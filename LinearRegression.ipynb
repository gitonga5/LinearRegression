
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f06c34ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print()\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10d1c9",
   "metadata": {},
   "source": [
    "##### 【Problem 1 】 Assumption function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74559330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _linear_hypothesis(self, X):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    次の形のndarray, shape (n_samples,)\n",
    "      線形の仮定関数による推定結果\n",
    "    \"\"\"\n",
    "    # Add bias term if needed\n",
    "    if not self.no_bias:\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    return X @ self.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0100a6",
   "metadata": {},
   "source": [
    "##### 【Problem 2 】 Fastest descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d54641f",
   "metadata": {},
   "source": [
    "1. _gradient_descent() Method Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1f63fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gradient_descent(self, X, error):\n",
    "    \"\"\"\n",
    "    勾配降下法によってパラメータを更新する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features or n_features+1 if bias included)\n",
    "        訓練データの特徴量（バイアス項を含む可能性あり）\n",
    "\n",
    "    error : ndarray, shape (n_samples,)\n",
    "        予測値と正解値の誤差（residual）\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # 勾配を計算（平均二乗誤差の勾配）\n",
    "    grad = (1 / n_samples) * X.T @ error\n",
    "\n",
    "    # パラメータを更新\n",
    "    self.coef_ -= self.lr * grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a6bb95",
   "metadata": {},
   "source": [
    "2. Updated fit() Method Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "544481ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, X_val=None, y_val=None):\n",
    "    \"\"\"\n",
    "    線形回帰を学習する。検証データが入力された場合はそれに対する損失も計算する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "        訓練データの特徴量\n",
    "\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        訓練データの正解値\n",
    "\n",
    "    X_val : ndarray, shape (n_samples, n_features), optional\n",
    "        検証データの特徴量\n",
    "\n",
    "    y_val : ndarray, shape (n_samples,), optional\n",
    "        検証データの正解値\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # バイアス項を加える（必要な場合）\n",
    "    if not self.no_bias:\n",
    "        X = np.c_[np.ones(n_samples), X]\n",
    "        if X_val is not None:\n",
    "            X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
    "\n",
    "    # パラメータ初期化（0で初期化）\n",
    "    self.coef_ = np.zeros(X.shape[1])\n",
    "\n",
    "    for i in range(self.iter):\n",
    "        # 仮定関数による予測\n",
    "        y_pred = self._linear_hypothesis(X)\n",
    "\n",
    "        # 損失（誤差）を計算\n",
    "        error = y_pred - y\n",
    "\n",
    "        # パラメータ更新\n",
    "        self._gradient_descent(X, error)\n",
    "\n",
    "        # 訓練損失を記録\n",
    "        self.loss[i] = np.mean(error**2)\n",
    "\n",
    "        # 検証損失があれば記録\n",
    "        if X_val is not None and y_val is not None:\n",
    "            val_pred = self._linear_hypothesis(X_val)\n",
    "            val_error = val_pred - y_val\n",
    "            self.val_loss[i] = np.mean(val_error**2)\n",
    "\n",
    "        # 出力（verbose=Trueのときのみ）\n",
    "        if self.verbose and i % 10 == 0:\n",
    "            print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f6d7c",
   "metadata": {},
   "source": [
    "3. Minor Fix to predict() Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0e998c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    \"\"\"\n",
    "    線形回帰を使い推定する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "        サンプル\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray, shape (n_samples,)\n",
    "        線形回帰による推定結果\n",
    "    \"\"\"\n",
    "    if not self.no_bias:\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "    return self._linear_hypothesis(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4941b9d",
   "metadata": {},
   "source": [
    "##### 【Problem 3 】 Estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7319e",
   "metadata": {},
   "source": [
    "Final Implementation of predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "688dd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    \"\"\"\n",
    "    線形回帰モデルによる推定を行う\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "        推定したいサンプルデータ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray, shape (n_samples,)\n",
    "        推定結果（予測値）\n",
    "    \"\"\"\n",
    "    return self._linear_hypothesis(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9019ec4f",
   "metadata": {},
   "source": [
    "Reminder: _linear_hypothesis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92baea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _linear_hypothesis(self, X):\n",
    "    if not self.no_bias:\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "    return X @ self.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912c37a",
   "metadata": {},
   "source": [
    "##### 【Problem 4 】 Average square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6143d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : ndarray, shape (n_samples,)\n",
    "        推定した値\n",
    "\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        正解値\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mse : float\n",
    "        平均二乗誤差\n",
    "    \"\"\"\n",
    "    mse = np.mean((y_pred - y) ** 2)\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ba1fe",
   "metadata": {},
   "source": [
    "【Problem 5 】 Purpose function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e249f",
   "metadata": {},
   "source": [
    "Step 1: Define a general-purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b693e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    目的関数（1/2 平均二乗誤差）の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : ndarray, shape (n_samples,)\n",
    "        推定値\n",
    "\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        正解値\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        目的関数の値（1/2 MSE）\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    return 0.5 * np.mean((y_pred - y) ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69b00f",
   "metadata": {},
   "source": [
    "Step 2: Modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f24f0125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression:\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if not self.no_bias:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "            if X_val is not None:\n",
    "                X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
    "\n",
    "        self.coef_ = np.zeros(X.shape[1])\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            error = y_pred - y\n",
    "\n",
    "            # Update parameters\n",
    "            self._gradient_descent(X, error)\n",
    "\n",
    "            # Record training loss\n",
    "            self.loss[i] = half_MSE(y_pred, y)\n",
    "\n",
    "            # Record validation loss if available\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_pred = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[i] = half_MSE(val_pred, y_val)\n",
    "\n",
    "            if self.verbose and i % 10 == 0:\n",
    "                print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0163d5",
   "metadata": {},
   "source": [
    "【Problem 6 】 Learning and estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcef395",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e714365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Training Loss = 19442791762.8549\n",
      "Iteration 10: Training Loss = 21453823019590057967138869487316500480.0000\n",
      "Iteration 20: Training Loss = 42081609754241736798101386585887912979638531093541117444761845760.0000\n",
      "Iteration 30: Training Loss = 82542951803567668653339334306399389320886354504202794673911321824683495694885111220632813568.0000\n",
      "Iteration 40: Training Loss = 161907753344900639629165167791409449687417507662480377303689715594897126816465092804602489751483533981263437881172557824.0000\n",
      "Iteration 50: Training Loss = 317581574445947558555692353206923877960591119036287396615132780181769277186199257177313754750665031112652663681904069705836771713589400309304655872.0000\n",
      "Iteration 60: Training Loss = 622935309421019072508752079287842178355717501663349061571548733477574497458348049173387666250225883875102673989604507118073777322807485386047613709534891050755977456780312576.0000\n",
      "Iteration 70: Training Loss = 1221885748253655281542287495896682127838879450425020078535435818223952176611081194760441060710842610883308032769032173856928199668990009172870851816750298005023426818818805237148034064292794841803259904.0000\n",
      "Iteration 80: Training Loss = 2396725244509009472832831453568374256042044205411854368477321771623300724310609561725705044090443239332108274822755698186731605954081492746604143176662767873953882929999204601495298194824326037984810104342018112381118610011062272.0000\n",
      "Iteration 90: Training Loss = 4701169406285858831046737473491862896854782865833589004875041368752810835443011074975449255080954475868691267476632540336579426836839898495055146816983336284197335501478972943902629467519912047789045778133230629322958808964139558070287856607444299528798208.0000\n",
      "Iteration 100: Training Loss = 9221329744505396300939297869880827200063327534674512648084938243894793806615989185948773538798090257729594240540050061685350635974897830927176944913456694602044961027930906854217414728317684977603123925587051226195396072584054017125630175265904807881956111789387079070845953686634496.0000\n",
      "Iteration 110: Training Loss = inf\n",
      "Iteration 120: Training Loss = inf\n",
      "Iteration 130: Training Loss = inf\n",
      "Iteration 140: Training Loss = inf\n",
      "Iteration 150: Training Loss = inf\n",
      "Iteration 160: Training Loss = inf\n",
      "Iteration 170: Training Loss = inf\n",
      "Iteration 180: Training Loss = inf\n",
      "Iteration 190: Training Loss = inf\n",
      "\n",
      "--- Comparison ---\n",
      "Scratch RMSE: inf\n",
      "Scikit-learn RMSE: 36061.396067378824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericg\\AppData\\Local\\Temp\\ipykernel_31576\\561437752.py:56: RuntimeWarning: overflow encountered in square\n",
      "  return 0.5 * np.mean((y_pred - y_true) ** 2)\n",
      "c:\\Users\\ericg\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "c:\\Users\\ericg\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:510: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ========== Scratch Linear Regression Class ==========\n",
    "\n",
    "class ScratchLinearRegression:\n",
    "    def __init__(self, num_iter=100, lr=0.01, no_bias=False, verbose=False):\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "\n",
    "    def _linear_hypothesis(self, X):\n",
    "        return X @ self.coef_\n",
    "\n",
    "    def _gradient_descent(self, X, error):\n",
    "        m = X.shape[0]\n",
    "        gradient = (X.T @ error) / m\n",
    "        self.coef_ -= self.lr * gradient\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if not self.no_bias:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "            if X_val is not None:\n",
    "                X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
    "\n",
    "        self.coef_ = np.zeros(X.shape[1])\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            error = y_pred - y\n",
    "            self._gradient_descent(X, error)\n",
    "\n",
    "            self.loss[i] = half_MSE(y_pred, y)\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_pred = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[i] = half_MSE(val_pred, y_val)\n",
    "\n",
    "            if self.verbose and i % 10 == 0:\n",
    "                print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.no_bias:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return self._linear_hypothesis(X)\n",
    "\n",
    "# ========== Loss Function ==========\n",
    "\n",
    "def half_MSE(y_pred, y_true):\n",
    "    return 0.5 * np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def RMSE(y_pred, y_true):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# ========== Load and Prepare House Prices Data ==========\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "X = data.select_dtypes(include=[np.number]).drop(columns=[\"Id\", \"SalePrice\"]).fillna(0)\n",
    "y = data[\"SalePrice\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ========== Train Scratch Model ==========\n",
    "\n",
    "scratch_model = ScratchLinearRegression(num_iter=200, lr=1e-7, no_bias=False, verbose=True)\n",
    "scratch_model.fit(X_train, y_train, X_val=X_test, y_val=y_test)\n",
    "y_pred_scratch = scratch_model.predict(X_test)\n",
    "\n",
    "# ========== Train Scikit-learn Model ==========\n",
    "\n",
    "sk_model = LinearRegression()\n",
    "sk_model.fit(X_train, y_train)\n",
    "y_pred_sk = sk_model.predict(X_test)\n",
    "\n",
    "# ========== Evaluation ==========\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(\"Scratch RMSE:\", RMSE(y_pred_scratch, y_test))\n",
    "print(\"Scikit-learn RMSE:\", RMSE(y_pred_sk, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0670c59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Training Loss = 19442791762.8549, Validation Loss = 19827002217.1022\n",
      "Iteration 10: Training Loss = 19442791752.7373, Validation Loss = 19827002208.1760\n",
      "Iteration 20: Training Loss = 19442791742.6197, Validation Loss = 19827002199.2497\n",
      "Iteration 30: Training Loss = 19442791732.5020, Validation Loss = 19827002190.3234\n",
      "Iteration 40: Training Loss = 19442791722.3844, Validation Loss = 19827002181.3971\n",
      "Iteration 50: Training Loss = 19442791712.2668, Validation Loss = 19827002172.4709\n",
      "Iteration 60: Training Loss = 19442791702.1492, Validation Loss = 19827002163.5446\n",
      "Iteration 70: Training Loss = 19442791692.0316, Validation Loss = 19827002154.6183\n",
      "Iteration 80: Training Loss = 19442791681.9140, Validation Loss = 19827002145.6921\n",
      "Iteration 90: Training Loss = 19442791671.7964, Validation Loss = 19827002136.7658\n",
      "Iteration 100: Training Loss = 19442791661.6788, Validation Loss = 19827002127.8395\n",
      "Iteration 110: Training Loss = 19442791651.5612, Validation Loss = 19827002118.9132\n",
      "Iteration 120: Training Loss = 19442791641.4436, Validation Loss = 19827002109.9870\n",
      "Iteration 130: Training Loss = 19442791631.3260, Validation Loss = 19827002101.0607\n",
      "Iteration 140: Training Loss = 19442791621.2083, Validation Loss = 19827002092.1344\n",
      "Iteration 150: Training Loss = 19442791611.0907, Validation Loss = 19827002083.2082\n",
      "Iteration 160: Training Loss = 19442791600.9731, Validation Loss = 19827002074.2819\n",
      "Iteration 170: Training Loss = 19442791590.8555, Validation Loss = 19827002065.3556\n",
      "Iteration 180: Training Loss = 19442791580.7379, Validation Loss = 19827002056.4293\n",
      "Iteration 190: Training Loss = 19442791570.6203, Validation Loss = 19827002047.5031\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the MSE function\n",
    "def half_MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    Mean squared error calculation (scaled by 1/2).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : ndarray, shape (n_samples,)\n",
    "        Predicted values\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        True values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mse : float\n",
    "        Half of the mean squared error\n",
    "    \"\"\"\n",
    "    return np.mean((y_pred - y) ** 2) / 2\n",
    "\n",
    "# ScratchLinearRegression class\n",
    "class ScratchLinearRegression:\n",
    "    def __init__(self, num_iter=100, lr=1e-6, no_bias=False, verbose=False):\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        self.loss = np.zeros(self.iter)  # Training loss\n",
    "        self.val_loss = np.zeros(self.iter)  # Validation loss\n",
    "\n",
    "    def _linear_hypothesis(self, X):\n",
    "        return X @ self.coef_\n",
    "\n",
    "    def _gradient_descent(self, X, error, clip_value=1.0):\n",
    "        m = X.shape[0]\n",
    "        gradient = (X.T @ error) / m\n",
    "        # Clip gradients to avoid excessively large updates\n",
    "        np.clip(gradient, -clip_value, clip_value, out=gradient)\n",
    "        self.coef_ -= self.lr * gradient\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if not self.no_bias:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "            if X_val is not None:\n",
    "                X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
    "\n",
    "        self.coef_ = np.zeros(X.shape[1])\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            # Training phase\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            error = y_pred - y\n",
    "            self._gradient_descent(X, error)\n",
    "\n",
    "            # Record training loss\n",
    "            self.loss[i] = half_MSE(y_pred, y)\n",
    "\n",
    "            # Record validation loss if available\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_pred = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[i] = half_MSE(val_pred, y_val)\n",
    "\n",
    "            # Verbose output every 10 iterations for debugging\n",
    "            if self.verbose and i % 10 == 0:\n",
    "                print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}, Validation Loss = {self.val_loss[i]:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.no_bias:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return self._linear_hypothesis(X)\n",
    "\n",
    "# Function to plot learning curve\n",
    "def plot_learning_curve(train_loss, val_loss):\n",
    "    \"\"\"\n",
    "    Plot the learning curve showing training and validation losses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_loss : ndarray\n",
    "        Array containing the training loss values.\n",
    "    val_loss : ndarray\n",
    "        Array containing the validation loss values.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss', linestyle='--')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example: Load and prepare data for the House Prices competition\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "X = data.select_dtypes(include=[np.number]).drop(columns=[\"Id\", \"SalePrice\"]).fillna(0)\n",
    "y = data[\"SalePrice\"].values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Scratch model with normalized features and smaller learning rate\n",
    "scratch_model = ScratchLinearRegression(num_iter=200, lr=1e-6, no_bias=False, verbose=True)\n",
    "scratch_model.fit(X_train_scaled, y_train, X_val=X_test_scaled, y_val=y_test)\n",
    "\n",
    "# Plot the learning curve\n",
    "plot_learning_curve(scratch_model.loss, scratch_model.val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b74594",
   "metadata": {},
   "source": [
    "##### 【Problem 8 】 (Advance Challenge) Removal of Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4be6df31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Training Loss = 19442791762.8549, Validation Loss = 19826401761.2824\n",
      "Iteration 10: Training Loss = 19436500798.5760, Validation Loss = 19820399642.3126\n",
      "Iteration 20: Training Loss = 19430214505.5478, Validation Loss = 19814401970.6906\n",
      "Iteration 30: Training Loss = 19423932878.2188, Validation Loss = 19808408740.9322\n",
      "Iteration 40: Training Loss = 19417655911.0451, Validation Loss = 19802419947.5602\n",
      "Iteration 50: Training Loss = 19411383598.4900, Validation Loss = 19796435585.1054\n",
      "Iteration 60: Training Loss = 19405115935.0243, Validation Loss = 19790455648.1056\n",
      "Iteration 70: Training Loss = 19398852915.1260, Validation Loss = 19784480131.1065\n",
      "Iteration 80: Training Loss = 19392594533.2806, Validation Loss = 19778509028.6608\n",
      "Iteration 90: Training Loss = 19386340783.9806, Validation Loss = 19772542335.3289\n",
      "Iteration 100: Training Loss = 19380091661.7260, Validation Loss = 19766580045.6786\n",
      "Iteration 110: Training Loss = 19373847161.0242, Validation Loss = 19760622154.2851\n",
      "Iteration 120: Training Loss = 19367607276.3896, Validation Loss = 19754668655.7308\n",
      "Iteration 130: Training Loss = 19361372002.3440, Validation Loss = 19748719544.6058\n",
      "Iteration 140: Training Loss = 19355141333.4166, Validation Loss = 19742774815.5074\n",
      "Iteration 150: Training Loss = 19348915264.1437, Validation Loss = 19736834463.0403\n",
      "Iteration 160: Training Loss = 19342693789.0688, Validation Loss = 19730898481.8166\n",
      "Iteration 170: Training Loss = 19336476902.7427, Validation Loss = 19724966866.4556\n",
      "Iteration 180: Training Loss = 19330264599.7234, Validation Loss = 19719039611.5842\n",
      "Iteration 190: Training Loss = 19324056874.5762, Validation Loss = 19713116711.8363\n",
      "Iteration 200: Training Loss = 19317853721.8735, Validation Loss = 19707198161.8535\n",
      "Iteration 210: Training Loss = 19311655136.1949, Validation Loss = 19701283956.2844\n",
      "Iteration 220: Training Loss = 19305461112.1273, Validation Loss = 19695374089.7849\n",
      "Iteration 230: Training Loss = 19299271644.2645, Validation Loss = 19689468557.0185\n",
      "Iteration 240: Training Loss = 19293086727.2077, Validation Loss = 19683567352.6557\n",
      "Iteration 250: Training Loss = 19286906355.5653, Validation Loss = 19677670471.3742\n",
      "Iteration 260: Training Loss = 19280730523.9525, Validation Loss = 19671777907.8593\n",
      "Iteration 270: Training Loss = 19274559226.9921, Validation Loss = 19665889656.8033\n",
      "Iteration 280: Training Loss = 19268392459.3138, Validation Loss = 19660005712.9057\n",
      "Iteration 290: Training Loss = 19262230215.5542, Validation Loss = 19654126070.8733\n",
      "Iteration 300: Training Loss = 19256072490.3573, Validation Loss = 19648250725.4203\n",
      "Iteration 310: Training Loss = 19249919278.3742, Validation Loss = 19642379671.2678\n",
      "Iteration 320: Training Loss = 19243770574.2628, Validation Loss = 19636512903.1443\n",
      "Iteration 330: Training Loss = 19237626372.6885, Validation Loss = 19630650415.7853\n",
      "Iteration 340: Training Loss = 19231486668.3234, Validation Loss = 19624792203.9339\n",
      "Iteration 350: Training Loss = 19225351455.8468, Validation Loss = 19618938262.3398\n",
      "Iteration 360: Training Loss = 19219220729.9450, Validation Loss = 19613088585.7602\n",
      "Iteration 370: Training Loss = 19213094485.3114, Validation Loss = 19607243168.9595\n",
      "Iteration 380: Training Loss = 19206972716.6465, Validation Loss = 19601402006.7091\n",
      "Iteration 390: Training Loss = 19200855418.6576, Validation Loss = 19595565093.7874\n",
      "Iteration 400: Training Loss = 19194742586.0592, Validation Loss = 19589732424.9803\n",
      "Iteration 410: Training Loss = 19188634213.5727, Validation Loss = 19583903995.0804\n",
      "Iteration 420: Training Loss = 19182530295.9266, Validation Loss = 19578079798.8877\n",
      "Iteration 430: Training Loss = 19176430827.8562, Validation Loss = 19572259831.2091\n",
      "Iteration 440: Training Loss = 19170335804.1039, Validation Loss = 19566444086.8588\n",
      "Iteration 450: Training Loss = 19164245219.4191, Validation Loss = 19560632560.6578\n",
      "Iteration 460: Training Loss = 19158159068.5580, Validation Loss = 19554825247.4343\n",
      "Iteration 470: Training Loss = 19152077346.2839, Validation Loss = 19549022142.0237\n",
      "Iteration 480: Training Loss = 19146000047.3670, Validation Loss = 19543223239.2681\n",
      "Iteration 490: Training Loss = 19139927166.5843, Validation Loss = 19537428534.0168\n",
      "Iteration 0: Training Loss = 19442791762.8549, Validation Loss = 19826726250.2297\n",
      "Iteration 10: Training Loss = 19439792589.1596, Validation Loss = 19823968758.1515\n",
      "Iteration 20: Training Loss = 19436797428.4193, Validation Loss = 19821215235.9446\n",
      "Iteration 30: Training Loss = 19433806275.2143, Validation Loss = 19818465678.0470\n",
      "Iteration 40: Training Loss = 19430819124.1323, Validation Loss = 19815720078.9042\n",
      "Iteration 50: Training Loss = 19427835969.7683, Validation Loss = 19812978432.9693\n",
      "Iteration 60: Training Loss = 19424856806.7245, Validation Loss = 19810240734.7032\n",
      "Iteration 70: Training Loss = 19421881629.6105, Validation Loss = 19807506978.5742\n",
      "Iteration 80: Training Loss = 19418910433.0431, Validation Loss = 19804777159.0586\n",
      "Iteration 90: Training Loss = 19415943211.6467, Validation Loss = 19802051270.6399\n",
      "Iteration 100: Training Loss = 19412979960.0526, Validation Loss = 19799329307.8095\n",
      "Iteration 110: Training Loss = 19410020672.8994, Validation Loss = 19796611265.0661\n",
      "Iteration 120: Training Loss = 19407065344.8333, Validation Loss = 19793897136.9163\n",
      "Iteration 130: Training Loss = 19404113970.5072, Validation Loss = 19791186917.8739\n",
      "Iteration 140: Training Loss = 19401166544.5818, Validation Loss = 19788480602.4607\n",
      "Iteration 150: Training Loss = 19398223061.7246, Validation Loss = 19785778185.2057\n",
      "Iteration 160: Training Loss = 19395283516.6105, Validation Loss = 19783079660.6456\n",
      "Iteration 170: Training Loss = 19392347903.9216, Validation Loss = 19780385023.3245\n",
      "Iteration 180: Training Loss = 19389416218.3471, Validation Loss = 19777694267.7940\n",
      "Iteration 190: Training Loss = 19386488454.5836, Validation Loss = 19775007388.6135\n",
      "Iteration 200: Training Loss = 19383564607.3345, Validation Loss = 19772324380.3497\n",
      "Iteration 210: Training Loss = 19380644671.3108, Validation Loss = 19769645237.5765\n",
      "Iteration 220: Training Loss = 19377728641.2304, Validation Loss = 19766969954.8758\n",
      "Iteration 230: Training Loss = 19374816511.8183, Validation Loss = 19764298526.8367\n",
      "Iteration 240: Training Loss = 19371908277.8069, Validation Loss = 19761630948.0556\n",
      "Iteration 250: Training Loss = 19369003933.9353, Validation Loss = 19758967213.1367\n",
      "Iteration 260: Training Loss = 19366103474.9503, Validation Loss = 19756307316.6913\n",
      "Iteration 270: Training Loss = 19363206895.6052, Validation Loss = 19753651253.3384\n",
      "Iteration 280: Training Loss = 19360314190.6608, Validation Loss = 19750999017.7042\n",
      "Iteration 290: Training Loss = 19357425354.8849, Validation Loss = 19748350604.4224\n",
      "Iteration 300: Training Loss = 19354540383.0523, Validation Loss = 19745706008.1341\n",
      "Iteration 310: Training Loss = 19351659269.9450, Validation Loss = 19743065223.4878\n",
      "Iteration 320: Training Loss = 19348782010.3518, Validation Loss = 19740428245.1392\n",
      "Iteration 330: Training Loss = 19345908599.0689, Validation Loss = 19737795067.7517\n",
      "Iteration 340: Training Loss = 19343039030.8993, Validation Loss = 19735165685.9957\n",
      "Iteration 350: Training Loss = 19340173300.6531, Validation Loss = 19732540094.5492\n",
      "Iteration 360: Training Loss = 19337311403.1474, Validation Loss = 19729918288.0974\n",
      "Iteration 370: Training Loss = 19334453333.2063, Validation Loss = 19727300261.3329\n",
      "Iteration 380: Training Loss = 19331599085.6610, Validation Loss = 19724686008.9554\n",
      "Iteration 390: Training Loss = 19328748655.3496, Validation Loss = 19722075525.6723\n",
      "Iteration 400: Training Loss = 19325902037.1173, Validation Loss = 19719468806.1979\n",
      "Iteration 410: Training Loss = 19323059225.8160, Validation Loss = 19716865845.2540\n",
      "Iteration 420: Training Loss = 19320220216.3049, Validation Loss = 19714266637.5696\n",
      "Iteration 430: Training Loss = 19317385003.4499, Validation Loss = 19711671177.8811\n",
      "Iteration 440: Training Loss = 19314553582.1240, Validation Loss = 19709079460.9319\n",
      "Iteration 450: Training Loss = 19311725947.2071, Validation Loss = 19706491481.4728\n",
      "Iteration 460: Training Loss = 19308902093.5860, Validation Loss = 19703907234.2619\n",
      "Iteration 470: Training Loss = 19306082016.1544, Validation Loss = 19701326714.0644\n",
      "Iteration 480: Training Loss = 19303265709.8129, Validation Loss = 19698749915.6527\n",
      "Iteration 490: Training Loss = 19300453169.4692, Validation Loss = 19696176833.8066\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define MSE function (unchanged)\n",
    "def half_MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    Mean squared error calculation (scaled by 1/2).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : ndarray, shape (n_samples,)\n",
    "        Predicted values\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        True values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mse : float\n",
    "        Half of the mean squared error\n",
    "    \"\"\"\n",
    "    return np.mean((y_pred - y) ** 2) / 2\n",
    "\n",
    "# ScratchLinearRegression Class with an option to remove bias term\n",
    "class ScratchLinearRegression:\n",
    "    def __init__(self, num_iter=100, lr=1e-6, no_bias=False, verbose=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "        num_iter : int\n",
    "            Number of iterations for gradient descent.\n",
    "        lr : float\n",
    "            Learning rate.\n",
    "        no_bias : bool\n",
    "            Whether to exclude the bias term.\n",
    "        verbose : bool\n",
    "            Whether to print the progress during training.\n",
    "        \"\"\"\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        self.loss = np.zeros(self.iter)  # To track training loss\n",
    "        self.val_loss = np.zeros(self.iter)  # To track validation loss\n",
    "\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the hypothesis (h_theta(x)).\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Input data (features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (n_samples,)\n",
    "            Predicted values (h_theta(x)).\n",
    "        \"\"\"\n",
    "        return X @ self.coef_\n",
    "\n",
    "    def _gradient_descent(self, X, error):\n",
    "        \"\"\"\n",
    "        Perform gradient descent to update the model parameters.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Input data (features).\n",
    "        error : ndarray, shape (n_samples,)\n",
    "            Difference between predicted and actual values.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        gradient = (X.T @ error) / m\n",
    "        self.coef_ -= self.lr * gradient\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Train the linear regression model using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Training data (features).\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Training labels (true values).\n",
    "        X_val : ndarray, shape (n_samples, n_features), optional\n",
    "            Validation data (features).\n",
    "        y_val : ndarray, shape (n_samples,), optional\n",
    "            Validation labels (true values).\n",
    "        \"\"\"\n",
    "        if not self.no_bias:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term (1) to X\n",
    "            if X_val is not None:\n",
    "                X_val = np.c_[np.ones(X_val.shape[0]), X_val]  # Add bias term to X_val\n",
    "\n",
    "        # Initialize coefficients to zeros (including the bias term if no_bias is False)\n",
    "        self.coef_ = np.zeros(X.shape[1])\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            # Training phase\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            error = y_pred - y\n",
    "            self._gradient_descent(X, error)\n",
    "\n",
    "            # Record training loss\n",
    "            self.loss[i] = half_MSE(y_pred, y)\n",
    "\n",
    "            # Record validation loss if available\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_pred = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[i] = half_MSE(val_pred, y_val)\n",
    "\n",
    "            # Verbose output every 10 iterations for debugging\n",
    "            if self.verbose and i % 10 == 0:\n",
    "                print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}, Validation Loss = {self.val_loss[i]:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using the trained linear regression model.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Input data (features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (n_samples,)\n",
    "            Predicted values.\n",
    "        \"\"\"\n",
    "        if not self.no_bias:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term if required\n",
    "        return self._linear_hypothesis(X)\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "# Load data (House Prices competition example)\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "X = data.select_dtypes(include=[np.number]).drop(columns=[\"Id\", \"SalePrice\"]).fillna(0)\n",
    "y = data[\"SalePrice\"].values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train models: One with bias and one without bias\n",
    "model_with_bias = ScratchLinearRegression(num_iter=500, lr=1e-5, no_bias=False, verbose=True)\n",
    "model_without_bias = ScratchLinearRegression(num_iter=500, lr=1e-5, no_bias=True, verbose=True)\n",
    "\n",
    "model_with_bias.fit(X_train_scaled, y_train, X_val=X_test_scaled, y_val=y_test)\n",
    "model_without_bias.fit(X_train_scaled, y_train, X_val=X_test_scaled, y_val=y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb46d90",
   "metadata": {},
   "source": [
    "##### 【Problem 9 】 (Advance Challenge) Multidimensional characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55e48385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Training Loss = 19442791762.8549, Validation Loss = 19826401761.2824\n",
      "Iteration 10: Training Loss = 19436500798.5760, Validation Loss = 19820399642.3126\n",
      "Iteration 20: Training Loss = 19430214505.5478, Validation Loss = 19814401970.6906\n",
      "Iteration 30: Training Loss = 19423932878.2188, Validation Loss = 19808408740.9322\n",
      "Iteration 40: Training Loss = 19417655911.0451, Validation Loss = 19802419947.5602\n",
      "Iteration 50: Training Loss = 19411383598.4900, Validation Loss = 19796435585.1054\n",
      "Iteration 60: Training Loss = 19405115935.0243, Validation Loss = 19790455648.1056\n",
      "Iteration 70: Training Loss = 19398852915.1260, Validation Loss = 19784480131.1065\n",
      "Iteration 80: Training Loss = 19392594533.2806, Validation Loss = 19778509028.6608\n",
      "Iteration 90: Training Loss = 19386340783.9806, Validation Loss = 19772542335.3289\n",
      "Iteration 100: Training Loss = 19380091661.7260, Validation Loss = 19766580045.6786\n",
      "Iteration 110: Training Loss = 19373847161.0242, Validation Loss = 19760622154.2851\n",
      "Iteration 120: Training Loss = 19367607276.3896, Validation Loss = 19754668655.7308\n",
      "Iteration 130: Training Loss = 19361372002.3440, Validation Loss = 19748719544.6058\n",
      "Iteration 140: Training Loss = 19355141333.4166, Validation Loss = 19742774815.5074\n",
      "Iteration 150: Training Loss = 19348915264.1437, Validation Loss = 19736834463.0403\n",
      "Iteration 160: Training Loss = 19342693789.0688, Validation Loss = 19730898481.8166\n",
      "Iteration 170: Training Loss = 19336476902.7427, Validation Loss = 19724966866.4556\n",
      "Iteration 180: Training Loss = 19330264599.7234, Validation Loss = 19719039611.5842\n",
      "Iteration 190: Training Loss = 19324056874.5762, Validation Loss = 19713116711.8363\n",
      "Iteration 200: Training Loss = 19317853721.8735, Validation Loss = 19707198161.8535\n",
      "Iteration 210: Training Loss = 19311655136.1949, Validation Loss = 19701283956.2844\n",
      "Iteration 220: Training Loss = 19305461112.1273, Validation Loss = 19695374089.7849\n",
      "Iteration 230: Training Loss = 19299271644.2645, Validation Loss = 19689468557.0185\n",
      "Iteration 240: Training Loss = 19293086727.2077, Validation Loss = 19683567352.6557\n",
      "Iteration 250: Training Loss = 19286906355.5653, Validation Loss = 19677670471.3742\n",
      "Iteration 260: Training Loss = 19280730523.9525, Validation Loss = 19671777907.8593\n",
      "Iteration 270: Training Loss = 19274559226.9921, Validation Loss = 19665889656.8033\n",
      "Iteration 280: Training Loss = 19268392459.3138, Validation Loss = 19660005712.9057\n",
      "Iteration 290: Training Loss = 19262230215.5542, Validation Loss = 19654126070.8733\n",
      "Iteration 300: Training Loss = 19256072490.3573, Validation Loss = 19648250725.4203\n",
      "Iteration 310: Training Loss = 19249919278.3742, Validation Loss = 19642379671.2678\n",
      "Iteration 320: Training Loss = 19243770574.2628, Validation Loss = 19636512903.1443\n",
      "Iteration 330: Training Loss = 19237626372.6885, Validation Loss = 19630650415.7853\n",
      "Iteration 340: Training Loss = 19231486668.3234, Validation Loss = 19624792203.9339\n",
      "Iteration 350: Training Loss = 19225351455.8468, Validation Loss = 19618938262.3398\n",
      "Iteration 360: Training Loss = 19219220729.9450, Validation Loss = 19613088585.7602\n",
      "Iteration 370: Training Loss = 19213094485.3114, Validation Loss = 19607243168.9595\n",
      "Iteration 380: Training Loss = 19206972716.6465, Validation Loss = 19601402006.7091\n",
      "Iteration 390: Training Loss = 19200855418.6576, Validation Loss = 19595565093.7874\n",
      "Iteration 400: Training Loss = 19194742586.0592, Validation Loss = 19589732424.9803\n",
      "Iteration 410: Training Loss = 19188634213.5727, Validation Loss = 19583903995.0804\n",
      "Iteration 420: Training Loss = 19182530295.9266, Validation Loss = 19578079798.8877\n",
      "Iteration 430: Training Loss = 19176430827.8562, Validation Loss = 19572259831.2091\n",
      "Iteration 440: Training Loss = 19170335804.1039, Validation Loss = 19566444086.8588\n",
      "Iteration 450: Training Loss = 19164245219.4191, Validation Loss = 19560632560.6578\n",
      "Iteration 460: Training Loss = 19158159068.5580, Validation Loss = 19554825247.4343\n",
      "Iteration 470: Training Loss = 19152077346.2839, Validation Loss = 19549022142.0237\n",
      "Iteration 480: Training Loss = 19146000047.3670, Validation Loss = 19543223239.2681\n",
      "Iteration 490: Training Loss = 19139927166.5843, Validation Loss = 19537428534.0168\n",
      "Iteration 0: Training Loss = 19442791762.8549, Validation Loss = 19813168858.2151\n",
      "Iteration 10: Training Loss = 19291397048.8465, Validation Loss = 19675843357.0500\n",
      "Iteration 20: Training Loss = 19143082787.8930, Validation Loss = 19540322344.1126\n",
      "Iteration 30: Training Loss = 18997731875.4063, Validation Loss = 19406566276.3868\n",
      "Iteration 40: Training Loss = 18855233963.1029, Validation Loss = 19274536919.3293\n",
      "Iteration 50: Training Loss = 18715484941.4091, Validation Loss = 19144197293.2858\n",
      "Iteration 60: Training Loss = 18578386469.1765, Validation Loss = 19015511622.3065\n",
      "Iteration 70: Training Loss = 18443845545.9679, Validation Loss = 18888445285.2515\n",
      "Iteration 80: Training Loss = 18311774122.6660, Validation Loss = 18762964769.0758\n",
      "Iteration 90: Training Loss = 18182088746.6028, Validation Loss = 18639037624.1955\n",
      "Iteration 100: Training Loss = 18054710237.8021, Validation Loss = 18516632421.8351\n",
      "Iteration 110: Training Loss = 17929563393.2814, Validation Loss = 18395718713.2657\n",
      "Iteration 120: Training Loss = 17806576716.6751, Validation Loss = 18276266990.8465\n",
      "Iteration 130: Training Loss = 17685682170.7249, Validation Loss = 18158248650.7840\n",
      "Iteration 140: Training Loss = 17566814950.4351, Validation Loss = 18041635957.5313\n",
      "Iteration 150: Training Loss = 17449913274.9160, Validation Loss = 17926402009.7517\n",
      "Iteration 160: Training Loss = 17334918196.1431, Validation Loss = 17812520707.7722\n",
      "Iteration 170: Training Loss = 17221773423.0369, Validation Loss = 17699966722.4616\n",
      "Iteration 180: Training Loss = 17110425159.4346, Validation Loss = 17588715465.4647\n",
      "Iteration 190: Training Loss = 17000821954.6652, Validation Loss = 17478743060.7325\n",
      "Iteration 200: Training Loss = 16892914565.5724, Validation Loss = 17370026317.2869\n",
      "Iteration 210: Training Loss = 16786655828.9449, Validation Loss = 17262542703.1663\n",
      "Iteration 220: Training Loss = 16682000543.4159, Validation Loss = 17156270320.4944\n",
      "Iteration 230: Training Loss = 16578905359.9899, Validation Loss = 17051187881.6246\n",
      "Iteration 240: Training Loss = 16477328680.4349, Validation Loss = 16947274686.3083\n",
      "Iteration 250: Training Loss = 16377230562.8563, Validation Loss = 16844510599.8422\n",
      "Iteration 260: Training Loss = 16278572633.8314, Validation Loss = 16742876032.1493\n",
      "Iteration 270: Training Loss = 16181318006.5485, Validation Loss = 16642351917.7508\n",
      "Iteration 280: Training Loss = 16085431204.4448, Validation Loss = 16542919696.5910\n",
      "Iteration 290: Training Loss = 15990878089.8862, Validation Loss = 16444561295.6724\n",
      "Iteration 300: Training Loss = 15897625797.4781, Validation Loss = 16347259111.4693\n",
      "Iteration 310: Training Loss = 15805642671.6307, Validation Loss = 16250995993.0808\n",
      "Iteration 320: Training Loss = 15714898208.0424, Validation Loss = 16155755226.0923\n",
      "Iteration 330: Training Loss = 15625362998.7921, Validation Loss = 16061520517.1129\n",
      "Iteration 340: Training Loss = 15537008680.7624, Validation Loss = 15968275978.9580\n",
      "Iteration 350: Training Loss = 15449807887.1389, Validation Loss = 15876006116.4491\n",
      "Iteration 360: Training Loss = 15363734201.7560, Validation Loss = 15784695812.8026\n",
      "Iteration 370: Training Loss = 15278762116.0781, Validation Loss = 15694330316.5814\n",
      "Iteration 380: Training Loss = 15194866988.6254, Validation Loss = 15604895229.1847\n",
      "Iteration 390: Training Loss = 15112025006.6688, Validation Loss = 15516376492.8505\n",
      "Iteration 400: Training Loss = 15030213150.0346, Validation Loss = 15428760379.1509\n",
      "Iteration 410: Training Loss = 14949409156.8727, Validation Loss = 15342033477.9552\n",
      "Iteration 420: Training Loss = 14869591491.2553, Validation Loss = 15256182686.8421\n",
      "Iteration 430: Training Loss = 14790739312.4823, Validation Loss = 15171195200.9411\n",
      "Iteration 440: Training Loss = 14712832445.9821, Validation Loss = 15087058503.1825\n",
      "Iteration 450: Training Loss = 14635851355.7039, Validation Loss = 15003760354.9404\n",
      "Iteration 460: Training Loss = 14559777117.9060, Validation Loss = 14921288787.0494\n",
      "Iteration 470: Training Loss = 14484591396.2540, Validation Loss = 14839632091.1799\n",
      "Iteration 480: Training Loss = 14410276418.1461, Validation Loss = 14758778811.5560\n",
      "Iteration 490: Training Loss = 14336814952.1925, Validation Loss = 14678717736.9999\n",
      "Mean Squared Error (Original Features): 19532216883.9056\n",
      "Mean Squared Error (Squared Features): 14607331030.2622\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ScratchLinearRegression Class and other necessary code as defined previously...\n",
    "\n",
    "def add_squared_features(X):\n",
    "    \"\"\"\n",
    "    Add squared features to the original feature set.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "        Input data (features).\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    ndarray, shape (n_samples, 2 * n_features)\n",
    "        Extended feature set including squared features.\n",
    "    \"\"\"\n",
    "    X_squared = np.square(X)  # Square each feature\n",
    "    return np.c_[X, X_squared]  # Combine original and squared features\n",
    "\n",
    "# Example Usage with Squared Features:\n",
    "# Load data (House Prices competition example)\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "X = data.select_dtypes(include=[np.number]).drop(columns=[\"Id\", \"SalePrice\"]).fillna(0)\n",
    "y = data[\"SalePrice\"].values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 1. Train Model with Original Features\n",
    "model_original = ScratchLinearRegression(num_iter=500, lr=1e-5, no_bias=False, verbose=True)\n",
    "model_original.fit(X_train_scaled, y_train, X_val=X_test_scaled, y_val=y_test)\n",
    "\n",
    "# 2. Train Model with Squared Features\n",
    "X_train_squared = add_squared_features(X_train_scaled)\n",
    "X_test_squared = add_squared_features(X_test_scaled)\n",
    "model_squared = ScratchLinearRegression(num_iter=500, lr=1e-5, no_bias=False, verbose=True)\n",
    "model_squared.fit(X_train_squared, y_train, X_val=X_test_squared, y_val=y_test)\n",
    "\n",
    "\n",
    "# You can also evaluate the performance by calculating the final MSE for both models\n",
    "y_pred_original = model_original.predict(X_test_scaled)\n",
    "y_pred_squared = model_squared.predict(X_test_squared)\n",
    "\n",
    "mse_original = half_MSE(y_pred_original, y_test)\n",
    "mse_squared = half_MSE(y_pred_squared, y_test)\n",
    "\n",
    "print(f\"Mean Squared Error (Original Features): {mse_original:.4f}\")\n",
    "print(f\"Mean Squared Error (Squared Features): {mse_squared:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ad311",
   "metadata": {},
   "source": [
    "##### 【Problem 10 】 (Advance Challenge) Update derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd2051",
   "metadata": {},
   "source": [
    "###### Problem 10: Derivation of Gradient Descent Update Rule\n",
    "\n",
    "##### To update parameters \\(\\theta_j\\) in linear regression using the **gradient descent method**, we start from the **loss function** and derive the update rule.\n",
    "\n",
    "##### Loss Function (Mean Squared Error)\n",
    "\n",
    "\\[\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(m\\): Number of training samples  \n",
    "- \\(h_\\theta(x^{(i)}) = \\theta^T x^{(i)}\\): Hypothesis/prediction  \n",
    "- \\(y^{(i)}\\): Ground truth for the \\(i\\)-th sample  \n",
    "\n",
    "##### Gradient of the Loss w.r.t. \\(\\theta_j\\)\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x^{(i)}_j\n",
    "\\]\n",
    "\n",
    "This gives the slope (gradient) of the loss function with respect to each weight.\n",
    "\n",
    "##### Gradient Descent Update Rule\n",
    "\n",
    "\\[\n",
    "\\theta_j := \\theta_j - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x^{(i)}_j\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\alpha\\): Learning rate  \n",
    "- The summation is the average gradient across all samples\n",
    "\n",
    "This formula tells us to subtract a scaled gradient from each \\(\\theta_j\\), moving downhill on the loss function surface.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd26534",
   "metadata": {},
   "source": [
    "##### 【Problem 10 】 (Advance Challenge) Update derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49ca7f",
   "metadata": {},
   "source": [
    "#####  Problem 11: Why Linear Regression Doesn’t Get Stuck in Local Minima\n",
    "\n",
    "Unlike more complex models, **linear regression is guaranteed to find a global optimum** when using gradient descent. Here's why:\n",
    "\n",
    "#####  The Cost Function is Convex\n",
    "\n",
    "The loss function:\n",
    "\n",
    "\\[\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
    "\\]\n",
    "\n",
    "is a **convex quadratic function** with respect to \\(\\theta\\).\n",
    "\n",
    "#####  Properties of Convex Functions:\n",
    "- They have **only one minimum** (no local minima).\n",
    "- Any local minimum is also the **global minimum**.\n",
    "- The surface of the cost function is a **bowl-shaped paraboloid**.\n",
    "\n",
    "#####  Implication for Gradient Descent\n",
    "\n",
    "Gradient descent, when applied to convex functions:\n",
    "- Will **always converge** to the global minimum, if:\n",
    "  - The learning rate is appropriate\n",
    "  - There is enough iteration\n",
    "- This ensures **stable, repeatable results** for linear regression\n",
    "\n",
    "#####  Optional Visualization Code (Python)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cost_function(theta):\n",
    "    return (theta - 3)**2 + 5  # Example quadratic\n",
    "\n",
    "theta_vals = np.linspace(-5, 10, 100)\n",
    "cost_vals = cost_function(theta_vals)\n",
    "\n",
    "plt.plot(theta_vals, cost_vals)\n",
    "plt.title('Convex Loss Function in Linear Regression')\n",
    "plt.xlabel('Theta')\n",
    "plt.ylabel('Cost J(θ)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
